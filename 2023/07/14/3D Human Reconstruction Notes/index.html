<!DOCTYPE html><html lang="zh-Hans" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>3D Human Reconstruction Notes | Colythme</title><meta name="description" content="重建个自己的avatar放metaverse里面跑  Human Face Reconstruction ReferencePIFuPIFu分为single-view surface reconstruction及multi-view surface reconstruction，但是实际上multi-view surface reconstruction就是对多个single-view的一个整"><meta name="keywords" content="DigitalHuman,Avatar,3D"><meta name="author" content="Colythme"><meta name="copyright" content="Colythme"><meta name="format-detection" content="telephone=no"><link rel="shortcut icon" href="https://i.loli.net/2020/07/01/HAkVZw1jYbBezxS.jpg"><link rel="canonical" href="http://colythme.github.io/2023/07/14/3D%20Human%20Reconstruction%20Notes/"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//fonts.googleapis.com" crossorigin="crossorigin"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><meta property="og:type" content="article"><meta property="og:title" content="3D Human Reconstruction Notes"><meta property="og:url" content="http://colythme.github.io/2023/07/14/3D%20Human%20Reconstruction%20Notes/"><meta property="og:site_name" content="Colythme"><meta property="og:description" content="重建个自己的avatar放metaverse里面跑  Human Face Reconstruction ReferencePIFuPIFu分为single-view surface reconstruction及multi-view surface reconstruction，但是实际上multi-view surface reconstruction就是对多个single-view的一个整"><meta property="og:image" content="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg"><meta property="article:published_time" content="2023-07-14T09:43:38.910Z"><meta property="article:modified_time" content="2023-07-14T09:53:33.986Z"><meta name="twitter:card" content="summary"><script>var activateDarkMode = function () {
  document.documentElement.setAttribute('data-theme', 'dark')
  if (document.querySelector('meta[name="theme-color"]') !== null) {
    document.querySelector('meta[name="theme-color"]').setAttribute('content', '#000')
  }
}
var activateLightMode = function () {
  document.documentElement.setAttribute('data-theme', 'light')
  if (document.querySelector('meta[name="theme-color"]') !== null) {
    document.querySelector('meta[name="theme-color"]').setAttribute('content', '#fff')
  }
}

var getCookies = function (name) {
  const value = `; ${document.cookie}`
  const parts = value.split(`; ${name}=`)
  if (parts.length === 2) return parts.pop().split(';').shift()
}

var autoChangeMode = '2'
var t = getCookies('theme')
if (autoChangeMode === '1') {
  var isDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches
  var isLightMode = window.matchMedia('(prefers-color-scheme: light)').matches
  var isNotSpecified = window.matchMedia('(prefers-color-scheme: no-preference)').matches
  var hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

  if (t === undefined) {
    if (isLightMode) activateLightMode()
    else if (isDarkMode) activateDarkMode()
    else if (isNotSpecified || hasNoSupport) {
      console.log('You specified no preference for a color scheme or your browser does not support it. I Schedule dark mode during night time.')
      var now = new Date()
      var hour = now.getHours()
      var isNight = hour <= 6 || hour >= 18
      isNight ? activateDarkMode() : activateLightMode()
    }
    window.matchMedia('(prefers-color-scheme: dark)').addListener(function (e) {
      if (Cookies.get('theme') === undefined) {
        e.matches ? activateDarkMode() : activateLightMode()
      }
    })
  } else if (t === 'light') activateLightMode()
  else activateDarkMode()
} else if (autoChangeMode === '2') {
  now = new Date()
  hour = now.getHours()
  isNight = hour <= 6 || hour >= 18
  if (t === undefined) isNight ? activateDarkMode() : activateLightMode()
  else if (t === 'light') activateLightMode()
  else activateDarkMode()
} else {
  if (t === 'dark') activateDarkMode()
  else if (t === 'light') activateLightMode()
}</script><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web&amp;display=swap"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"We didn't find any results for the search: ${query}"}},
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: undefined,
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  bookmark: {
    message_prev: 'Press',
    message_next: 'to bookmark this page'
  },
  runtime_unit: 'days',
  runtime: true,
  copyright: {"limitCount":50,"languages":{"author":"Author: Colythme","link":"Link: ","source":"Source: Colythme","info":"Copyright is owned by the author. For commercial reprints, please contact the author for authorization. For non-commercial reprints, please indicate the source."}},
  ClickShowText: undefined,
  medium_zoom: false,
  fancybox: true,
  Snackbar: {"bookmark":{"message_prev":"Press","message_next":"to bookmark this page"},"chs_to_cht":"Traditional Chinese Activated Manually","cht_to_chs":"Simplified Chinese Activated Manually","day_to_night":"Dark Mode Activated Manually","night_to_day":"Light Mode Activated Manually","bgLight":"#49b1f5","bgDark":"#121212","position":"bottom-left"},
  justifiedGallery: {
    js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
    css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
  },
  baiduPush: false,
  highlightCopy: true,
  highlightLang: true,
  isPhotoFigcaption: false,
  islazyload: true,
  isanchor: true    
}</script><script id="config_change">var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isSidebar: true,
  postUpdate: '2023-07-14 02:53:33'
}</script><noscript><style>
#nav {
  opacity: 1
}
.justified-gallery img {
  opacity: 1
}
</style></noscript><style type="text/css">#toggle-sidebar {bottom: 80px}</style><meta name="generator" content="Hexo 5.4.0"></head><body><div id="loading-box"><div class="loading-left-bg"></div><div class="loading-right-bg"></div><div class="spinner-box"><div class="configure-border-1"><div class="configure-core"></div></div><div class="configure-border-2"><div class="configure-core"></div></div><div class="loading-word">Loading...</div></div></div><div id="mobile-sidebar"><div id="menu_mask"></div><div id="mobile-sidebar-menus"><div class="mobile_author_icon"><img class="avatar-img" data-lazy-src="https://i.loli.net/2020/07/01/VbugJ9i4FUkIsoE.jpg" onerror="onerror=null;src='/img/404_2.jpeg'" alt="avatar"/></div><div class="mobile_post_data"><div class="mobile_data_item is-center"><div class="mobile_data_link"><a href="/archives/"><div class="headline">Articles</div><div class="length_num">89</div></a></div></div><div class="mobile_data_item is-center">      <div class="mobile_data_link"><a href="/tags/"><div class="headline">Tags</div><div class="length_num">80</div></a></div></div><div class="mobile_data_item is-center">     <div class="mobile_data_link"><a href="/categories/"><div class="headline">Categories</div><div class="length_num">25</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archive</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tag</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Category</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> Repose</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/gallery/"><i class="fa-fw fas fa-image"></i><span> Gallery</span></a></li><li><a class="site-page" href="/music/"><i class="fa-fw fas fa-music"></i><span> Music</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div></div></div><div id="body-wrap"><div id="sidebar"><i class="fas fa-arrow-right on" id="toggle-sidebar"></i><div class="sidebar-toc"><div class="sidebar-toc__title">Catalog</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar">     </div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Human-Face-Reconstruction-Reference"><span class="toc-number">1.</span> <span class="toc-text">Human Face Reconstruction Reference</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#PIFu"><span class="toc-number">1.1.</span> <span class="toc-text">PIFu</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#U-Net"><span class="toc-number">1.2.</span> <span class="toc-text">U-Net</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#I-M-Avatar"><span class="toc-number">1.3.</span> <span class="toc-text">I M Avatar</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Definition"><span class="toc-number">1.3.1.</span> <span class="toc-text">Definition</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#ARCH"><span class="toc-number">1.4.</span> <span class="toc-text">ARCH</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Def"><span class="toc-number">1.4.1.</span> <span class="toc-text">Def</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Contribution"><span class="toc-number">1.4.2.</span> <span class="toc-text">Contribution</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Process"><span class="toc-number">1.4.3.</span> <span class="toc-text">Process</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#ARCH-1"><span class="toc-number">1.5.</span> <span class="toc-text">ARCH++</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Contribution-1"><span class="toc-number">1.5.1.</span> <span class="toc-text">Contribution</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Definition-1"><span class="toc-number">1.5.2.</span> <span class="toc-text">Definition</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#SMPLicit"><span class="toc-number">1.6.</span> <span class="toc-text">SMPLicit</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Definition-2"><span class="toc-number">1.6.1.</span> <span class="toc-text">Definition</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Process-1"><span class="toc-number">1.6.2.</span> <span class="toc-text">Process</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#ARAH"><span class="toc-number">1.7.</span> <span class="toc-text">ARAH</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Definition-3"><span class="toc-number">1.7.1.</span> <span class="toc-text">Definition</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Process-2"><span class="toc-number">1.7.2.</span> <span class="toc-text">Process</span></a></li></ol></li></ol></li></ol></div></div></div><header class="post-bg" id="page-header" style="background-image: url(https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg)"><nav id="nav"><span class="pull-left" id="blog_name"><a class="blog_title" id="site-name" href="/">Colythme</a></span><span class="pull-right menus"><div id="search_button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> Search</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archive</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tag</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Category</span></a></div><div class="menus_item"><a class="site-page" href="javascript:void(0);"><i class="fa-fw fas fa-list"></i><span> Repose</span><i class="fas fa-chevron-down expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/gallery/"><i class="fa-fw fas fa-image"></i><span> Gallery</span></a></li><li><a class="site-page" href="/music/"><i class="fa-fw fas fa-music"></i><span> Music</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> Link</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> About</span></a></div></div><span class="toggle-menu close"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></span></span></nav><div id="post-info"><div id="post-title"><div class="posttitle">3D Human Reconstruction Notes</div></div><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2023-07-14T09:43:38.910Z" title="Created 2023-07-14 02:43:38">2023-07-14</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2023-07-14T09:53:33.986Z" title="Updated 2023-07-14 02:53:33">2023-07-14</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Machine-Learning/">Machine Learning</a></span></div><div class="meta-secondline"> <span class="post-meta-separator">|</span><span class="post-meta-pv-cv"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post View:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout_post" id="content-inner"><article id="post"><div class="post-content" id="article-container"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/hint.css/2.4.1/hint.min.css"><blockquote>
<p>重建个自己的avatar放metaverse里面跑</p>
</blockquote>
<h1 id="Human-Face-Reconstruction-Reference"><a href="#Human-Face-Reconstruction-Reference" class="headerlink" title="Human Face Reconstruction Reference"></a>Human Face Reconstruction Reference</h1><h2 id="PIFu"><a href="#PIFu" class="headerlink" title="PIFu"></a>PIFu</h2><p><em>PIFu</em>分为<em>single-view surface reconstruction</em>及<em>multi-view surface reconstruction</em>，但是实际上<em>multi-view surface reconstruction</em>就是对多个<em>single-view</em>的一个整合(比如简单粗暴的取平均值之类的)，故不多赘述。</p>
<p><em>PIFu</em>，全称<em>Pixel-Aligned Implicit Function</em>，顾名思义，它与之前的做human digitalization的算法的不同之处在于两点，一个是<em>Pixel-Aligned</em>，一个是<em>implicit function</em>。关于<em>pixel-aligned</em>，之前的算法大多都是<em>voxel-aligned</em>，也就是将一个三维的数据，即<em>voxel</em>，放到model中训练，显然，这费时又费空间，而<em>PIFu</em>只先关注二维数据，再利用<em>implicit function</em>的优势，将深度参数$z(X)$放进隐式函数中求解。</p>
<p>首先声明一下<em>implicit function</em> $f$的定义，<br>$$<br>f(F(x), z(X)) = s : s \in R<br>$$<br>其中，$x$表示一个二维点，$X$表示一个三维点，并且$x = \pi (X)$，表示$x$是$X$的一个二维projection，$z(X)$上面有说过，存储是$X$的深度参数，$F(x) = g(I(x))$是一个函数，其中$g(\cdot)$是一个<em>encoder</em>，之后由神经网络训练得到。而$f(\cdot)$得到的值$s$是个<em>binary</em>，即只有$0$和$1$，分别表示该点在mesh surface的外面和里面。</p>
<center><font color = "grey">fig. 1   structure pic from PIFu essay</font></center>

<p><em>fig. 1<em>是</em>PIFu<em>的流程图，如图所示，对于</em>surface reconstruction<em>，在计算机得到图像之后，将其放入</em>image encoder<em>，得到其特征向量$F_V(x)$，注意这里的下标$V$仅是用于将</em>surface<em>和</em>texture reconstruction<em>得到的特征向量区分开，即这依旧表示的是上文公式的$F(\cdot)$。得到必要的$F_V(x)$、$z(X)$作为参数后，便可以训练隐式函数$f_V(\cdot)$了， *loss function</em>为<br>$$<br>\mathcal L_V = \frac1n\sum\limits_{i = 1}^n |f_v(F_V(x_i), z(X_i)) - f^</em>_v(X_i)^2|<br>$$<br>其中，$f^*_v(X_i)$指的是由给定训练集中的训练数据可以得到的$f(\cdot)$函数。</p>
<p>知道了训练方法，那么现在问题就是如何获取训练样本，毕竟对于一个模型，我们可以从它身上取的点是无数个。一个最简单的想法就是，在模型的bounding box里随机取样，然而<em>PIFu<em>研究者经过实验，证明这种方式并不是很理想，他们决定再加上另一种采样方式: 随机在模型表面采样，并且给予其扰动，该扰动距离符合</em>Gauss Function</em> $\mathcal N(0, \sigma)(\sigma = 0.5cm)$。最终研究者确定在这两种取样方法比例为$1: 16$时为最佳。</p>
<p>解决了<em>Surface Reconstruction</em>，<em>Texture Reconstruction</em>同理，在其基础上，修改其$f(\cdot)$函数的表示方法，即$f_c(\cdot) = RGB$，并且更改损失函数为<br>$$<br>\mathcal L_C = \frac1n\sum\limits_{i = 1}^n |f_c(F_C(x’<em>i, F_V), X’</em>{i, z}) - C(X_i)|<br>$$<br>其中，$X’_i = X_i + \epsilon, \epsilon \sim \mathcal N(0, d), d = 1.0cm$，$x_i’ = \pi (X_i’)$</p>
<p>之所以<em>loss function</em>需要改成这样，是因为若保持原样，相当于在<em>surface</em>特征上又加上一个<em>color</em>的特征，而这样一般来讲，并且研究者经过实验，发现容易<em>overfitting</em>，那么不如在原有已得特征$F_V$的基础上加以改进，这样就可以省掉一个特征了，即$F_C (x_i’, F_V)$。而添加offset是为了让得到颜色的范围更广一些，毕竟我们无法对那无数个点都做训练，所以只能够让一个颜色存在于一定范围内来实现<em>texture inference</em>。</p>
<p>之后使用<em>Marching Cube</em>方法实现模型重建。</p>
<p>更细节一些，求解$F(\cdot)$的<em>encoder</em>使用的是<em>Stacked Hourglass Network</em>，而训练$f(\cdot)$使用的是多重$MLP$。</p>
<h2 id="U-Net"><a href="#U-Net" class="headerlink" title="U-Net"></a>U-Net</h2><p><em>U-Net</em> 一般是用来处理<em>semantic segmentation</em>类问题用的，而此类问题即是指将存在于一张图像的不同物体标注出来，其最终呈现效果实际上有点类似于<em>thermal image</em>，但是又不尽相同。传统处理<em>semantic segmentation</em>的方法一般都是通过<em>CV</em>实现，比如基于<em>threshold</em>或者<em>edge detection</em>的实现，之后Jonathan Long等人提出了基于<em>FCN(Fully Convolutional Network)</em>的实现，2015年，在<em>U-Net</em>这篇论文中，它基于<em>FCN</em>，提出一种能够适应小训练集的网络方案。</p>
<p>首先说说<em>FCN</em>，其实<em>FCN</em>和<em>CNN</em>的网络结构基本一致，泛泛而谈，只是将最后的<em>Fully Connected Layer</em>改成了一个全卷积网络，实际上顾名思义，<em>FCN</em>本身即全部是由卷积组成的。那么为什么要做这样的处理？因为在<em>CNN</em>中，要通过<em>FCL</em>，我们必须将之前的tensor全部化为一个$1 \times n$的vector，也就是说，最终我们得到的只有对图像全局的一个分类参数，而若我们要做<em>semantic segmentation</em>，显然这样是行不通的，那么此时将<em>FCL</em>改为全卷积，我们就可以获得每个pixel的分类信息，从而获取全局的分割信息。</p>
<p><em>Subsampling</em>的过程实际上是提取特征，level较低的layer用于提取小特征，level较高的layer用于抽象全局特征，即类似于classification的特征就是由这一块得到的，因为其<em>receptive field</em>较大，但是这也造成了其<em>resolution</em>较低。那么经过<em>subsampling</em>后，可以提高<em>resolution</em>并还原特征。其中，<em>FCN</em>在与前文的联系上采用<em>skip-layer</em>中逐pixel做加法的方法，即类似于<em>fig. 2</em>中的灰色箭头。</p>
<p>了解了<em>FCN</em>，<em>U-Net</em>理解起来就比较容易了，generally speaking，不去细究其内部网络详情，一如每个kernel的大小之类的，它实际上就是在与context连接的部分将逐pixel相加改为直接重叠，即将两个tensor重叠在一起，形成一个新的tensor，再进行下一步处理。</p>
<img src= "/img/loading.gif" data-lazy-src="https://s1.ax1x.com/2022/09/01/v5sKzt.png" style="zoom:40%;" />

<center><font color = "grey">fig. 2   network structure pic from U-Net essay</font></center>

<p>至于<em>U-Net</em>为何能在只有小训练集的时候也能很好的训练出一个不错的model，这归功于<em>Overlap-tile strategy</em>，实际上就是将一个大的图片分为很多较小的训练样本，那么对于关键词<em>overlap-tile</em>，这实际上就是扩大<em>padding</em>，来让我们选定的小范围区域的边缘部分也能被计入训练集进行训练。</p>
<h2 id="I-M-Avatar"><a href="#I-M-Avatar" class="headerlink" title="I M Avatar"></a>I M Avatar</h2><h3 id="Definition"><a href="#Definition" class="headerlink" title="Definition"></a>Definition</h3><p>PCA: 一种将多维变量在尽可能保留featrue的情况下降维的方法。</p>
<h2 id="ARCH"><a href="#ARCH" class="headerlink" title="ARCH"></a>ARCH</h2><p>Animatable Reconstruction of Clothed Humans</p>
<h3 id="Def"><a href="#Def" class="headerlink" title="Def"></a>Def</h3><ul>
<li><p>rigging：指的应当是建骨架的过程</p>
</li>
<li><p>![image-20221230103831196](/Users/colythme/Library/Application Support/typora-user-images/image-20221230103831196.png)</p>
</li>
<li><p>cv提到语义，一般指的是图像中像素点或目标本身的含义、性质等。</p>
<p>  比如cv的语义分割，指的是按照目标的种类性质进行图像分割，同一种物体分为一类。实际就是对每个像素点做分类任务。</p>
<p>  SemDF is a vector field represented by a vector-valued function V that accomplishes the transformation.v</p>
</li>
<li><p>SemS S = {(p, sp) : p ∈ R3} is a space consist- ing of 3D points where each point p ∈ S is associated to semantic information sp enabling the transformation opera- tion. SemDF is a vector field represented by a vector-valued function V that accomplishes the transformation,</p>
</li>
<li><p>pose-normalization我觉得指的应当是将project后得到的3D model normalize为基础动作或表情</p>
</li>
<li><p>In computer vision and graphics, 3D human models have been widely represented by a kinematic structure mimick- ing the anatomy that serves to control the pose, and a surface mesh that represents the human shape and geometry. Skin- ning is the transformation that deforms the surface given the pose.  It is parameterized by skinning weights that individ- ually influence body part transformations.</p>
<p>  其中，kinematic structure指的是运动学结构，即通过关节等的运动来表达。</p>
<p>  skinning是根据skinning wieghts来构建的，即比如一个手的weight是多少，我就让他倾斜多少度等（当然这个例子只是方便理解，实际并不一定是这样）。Skinning is the process of binding the actual <a target="_blank" rel="noopener" href="http://www.digitaltutors.com/subject/3d-tutorials"><a target="_blank" rel="noopener" href="http://www.digitaltutors.com/subject/3d-tutorials">3D</a></a> mesh to the joint setup you created.</p>
</li>
<li><p>weight: 一个joint可以管到一片区域，但是很明显，对于某个特定位置，可能会影响到该点的joint对他的影响程度是不同的，这也就有了weight的概念，即一个weight $w_{i, p}$表示某个joint $i$对某个点$p$的影响程度。而若对于某个点$p$，他所拥有的weight和为1的话，则称这个weight是被normalized的。</p>
<p>  那么LBS实际上就是对于一个点$p$，先算出每个joint对其进行linear transformation之后这个点应该在哪里，之后再将求出的每个点按照各自joint对$p$的权重乘起来并累加，类似于加权平均，得到最终$p$在经过某个变换应该所在的位置。</p>
</li>
<li><img src= "/img/loading.gif" data-lazy-src="/Users/colythme/Library/Application Support/typora-user-images/image-20221231103922306.png" alt="image-20221231103922306" style="zoom:67%;" />

<p>  可以发现对于点$v_i$，求$v_i’$实际上就是对于每个joint，累加其对于$v_i$的影响权重$w_{k, i}$乘上（经过canonical rest pose纠正过的vector仿射变换乘以$v_i$），实际上和LBS的步骤差不多。</p>
</li>
<li><p>Occupacy map中的某个点$p$的occupacy $o_p$是个0到1之间的数，表示该点被占据的概率是多少。</p>
</li>
</ul>
<p>Dataset: Our <strong>training</strong> dataset is composed of 375 3D scans from the RenderPeople dataset, and 207 3D scans from the AXYZ dataset. Our <strong>test</strong> dataset contains 64 scans from the RenderPeople dataset, 207 scans from the AXYZ dataset, 26 scans from the BUFF dataset, and 2D images from the DeepFashion dataset, representing clothed people with a large variety of complex clothing.</p>
<p>For each 3D scan, we pro- duce 360 images by rotating a camera around the vertical axis with intervals of 1 degree.</p>
<p>3D scan -&gt; fit a rigged 3D body template to the scan mesh (detect regions of self-contact and topology changes and cut the mesh before pose-normalization) -&gt; do pose-normalization</p>
<p>Comparison: We reconstruct the results on the same test set and repose them back to the original poses of the input images and compare the reconstructions with the ground truth surfaces in the original poses.</p>
<p>We report the average point-to-surface Euclidean distance (P2S) in centimeters, the Chamfer distance in centimeters, and the L2 normal reprojection error in Tab. 1.</p>
<p>Canonical Space: <img src= "/img/loading.gif" data-lazy-src="/Users/colythme/Library/Application Support/typora-user-images/image-20230101115134759.png" alt="image-20230101115134759" style="zoom:50%;" /></p>
<h3 id="Contribution"><a href="#Contribution" class="headerlink" title="Contribution"></a>Contribution</h3><p><em>we introduce the Semantic Space (SemS) and Semantic Deformation Field (SemDF) to handle implicit function representation of clothed humans in arbitrary poses.</em></p>
<p>那么他使用的方法就是先给定一个template model，假定他的semantic space是$S = \{(p, \{w_{i, p}\}^{N_K}<em>{i = 1}) : p \in \mathbb{R}^3\}$。那么根据skinning的方法比如LBS，对于一个任意pose的model，其上一点$p’$都肯定可以被template model上某个点$p$转化到，换言之，$p’$ is a linear combination of $\{w</em>{i, p}\}$。</p>
<p>那么问题就到了如何求$\mathcal{V}$。</p>
<p><em>we propose opacity-aware differentiable rendering to refine our human representation via Granular Render-and-Compare</em></p>
<p><em>we demonstrate how reconstructed avatars can directly be rigged and skinned for animation. In addition, we learn per-pixel normals to obtain high-quality surface details, and surface albedo for relighting applications.</em></p>
<p>![image-20230102173751432](/Users/colythme/Library/Application Support/typora-user-images/image-20230102173751432.png)</p>
<h3 id="Process"><a href="#Process" class="headerlink" title="Process"></a>Process</h3><p><em>ARCH contains three components, after 3D body es- timation by [51] (see Fig. 2): pose-normalization using Semantic Space (SemS) and Semantic Deformation Field (SemDF), implicit surface reconstruction, and refinement using a differentiable renderer by Granular Render-and- Compare (see Sec. 3.4).</em></p>
<h2 id="ARCH-1"><a href="#ARCH-1" class="headerlink" title="ARCH++"></a>ARCH++</h2><h3 id="Contribution-1"><a href="#Contribution-1" class="headerlink" title="Contribution"></a>Contribution</h3><p><em>First, we introduce an end-to-end point based geometry en- coder to better describe the semantics of the underlying 3D human body, in replacement of previous hand-crafted features.</em></p>
<p><em>Second, in order to address the occupancy ambiguity caused by topological changes of clothed humans in the canonical pose, we propose a co-supervising framework with cross-space consistency to <strong>jointly</strong> estimate the occupancy in both the posed and canonical spaces.</em></p>
<p>这里的joint实际上就是将posed space和canonical space的点$p_a, p_b$合起来看，同时假设他们的occupacy为$o_a, o_b$，那么组成一个四维pair $(p_a, p_b, o_a, o_b)$，然后在marching cubes找isosurface的时候找$o_a = o_b = \tau$。实际上这样就某种程度避免了手在袋子里导致的reconstruction失误，因为$o_a, o_b$分别是小于0和大于0的。</p>
<p><em>Last, we use image-to-image translation networks to further refine detailed geometry and texture on the reconstructed surface, which improves the fidelity and consistency across arbitrary viewpoints.</em></p>
<h3 id="Definition-1"><a href="#Definition-1" class="headerlink" title="Definition"></a>Definition</h3><p>end-to-end: 深度学习提供了一种端到端的学习范式，整个学习的流程并不进行人为的子问题划分，而是完全交给深度学习模型直接学习从原始数据到期望输出的映射。对深度模型而言，其输入数据是未经任何人为加工的原始样本形式，后续则是堆叠在输入层上的众多操作层，这些操作层整体可以看作一个复杂的函数FCNN， 最终的损失函数由数据损失data loss和模型参数的正则化损失（regularization loss)共同组成, 模型深度的训练则是在最终损失驱动下对模型进行参数更新并将误差反向传播至网络各层。</p>
<p>PointNet++: 做<strong>Point Set Segmentation and Classification</strong></p>
<h2 id="SMPLicit"><a href="#SMPLicit" class="headerlink" title="SMPLicit"></a>SMPLicit</h2><h3 id="Definition-2"><a href="#Definition-2" class="headerlink" title="Definition"></a>Definition</h3><p>SDF: 首先，我们可以用SDF（Signed Distance Function）这样一个函数来隐式地表示一个三维物体，输入是空间中点的三维坐标，输出是这个点离我们想表示的物体表面的最近距离，如果在外部就是正，内部就是负。显然只要SDF找的好，从理论上来说，我们就能够简单粗暴地表示任意复杂且连续的物体，这也是物体的隐式表示方式与用点云、体素、网格等表示方式相比最大的好处。</p>
<p>这样就万事大吉了吗？并没有，因为如果用这种方法，并不是数据驱动的，而更像是一种数学的方法，我们每次进行重建的时候都得重新训练一个神经网络得到表示这个物体的专属SDF，比如给我一个轿车的若干采样点，我给训练出来了，你要是再给我一个卡车的采样点，我还得重新训练，虽说这样我们根本不需要数据集，但是缺点显而易见：我是希望在神经网络中引入对三维数据集的一些先验来辅助拟合，以便于更好地进行三维重建的，比如你就给我8个采样点，分别代表正方体的8个顶点，如果不引入数据集（如汽车数据集）的先验的话，最终拟合出来的东西肯定就是一个正方体（或者是个球体）之类的东西，而不是像个汽车的样子。</p>
<p>假定数据集中的某个数据被编码成latent code进行表示，这样我将这个latent code和三维坐标同时丢入神经网络查询得到sdf值，其实就能够得到某个具体的三维物体的SDF函数表示了</p>
<p>而有了SDF表示，我们就可以通过ray marching来render。</p>
<h3 id="Process-1"><a href="#Process-1" class="headerlink" title="Process"></a>Process</h3><p>SMPLicit本质上是要独立生成SMPL模型表面的衣服，而不用生成human body本身。</p>
<p>Clothing Cut: 这个clothing cut指的不是衣服裁剪问题，而是defined as the body area occluded by clothing。 for each garment-body pair in the training set, we compute a UV body <em>occlusion image</em> denoted as $U$. That is, we set every pixel in the SMPL body UV map to 1 if the corresponding body vertex is occluded by the garment, and 0 otherwise。然后再用一个image encoder从$U$ map到latent vector $z_{cut}$。注意，$U$是根据ground truth算出来的。</p>
<p>Clothing Styles: 我认为他使用DeepSDF去做$z_{style}$的拟合，本质上是因为每个衣服的特征是不同的，而DeepSDF的功能就是可以找不不同物体的SDF，那么这个$z_{style}$实际上是用来表示衣服的SDF的？</p>
<p>Body Shape: 由于在不同shape下衣服的表现是不同的，比如胖的人衣服就会膨胀起来，那么还要针对shape来进行编码训练。所以$P_{\beta}$本质上是表达衣服表面和body中vertex cluster的关系的参数，然后他记录的是衣服和body的距离（广义理解的话）。而$P_{\beta}$是$\mathbb{R}^3$中的一个3D点$p$ map的结果。</p>
<p>Output Representation: 实际上就是一个decoder network $C$，把$(P_{\beta}, z)$解码成为predicted unsigned distance $D (p)$。</p>
<p>Training</p>
<p>Loss: 可以理解为$D (p)$的prediction和ground truth的作差。</p>
<p>但是实际上上述操作都是在T-pose的前提下进行的，所以不能直接应用于3D reconstruction。</p>
<p>在3D human reconstruction的应用：</p>
<p>We first detect people and obtain an estimate of each person’s pose and shape [52], as well as a 2D cloth semantic segmentation [67]. We then fit SMPLicit to every detection to obtain layered 3D clothing.</p>
<p>接着在T-pose的SMPL模型上均匀采样，并且将这些sample point deform到当前pose下他们应当在的位置，再删除那些occluded的点。假定这些点为$p$，再将它们投影到2D平面，和原照片比对，如果点在clothes的semantic segmentation出来的块内则$s_p = 1$，反之为0。之后由损失函数处理。</p>
<h2 id="ARAH"><a href="#ARAH" class="headerlink" title="ARAH"></a>ARAH</h2><h3 id="Definition-3"><a href="#Definition-3" class="headerlink" title="Definition"></a>Definition</h3><p>Differentiable Rendering: 三维的物体渲染成二维图像的时候，其实本质上是进行了一系列矩阵变换，插值等操作，这和神经网络有一定的相似之处，渲染相当于前向传播，得到渲染图，而渲染图和输入图像相比较可以定义loss，从而进行反向传播，去优化三维物体的形状与纹理，从而实现基于单张图像的三维重建，并且不再受3D数据集依赖。</p>
<h3 id="Process-2"><a href="#Process-2" class="headerlink" title="Process"></a>Process</h3><p>他的关键在于root-finding algorithm的改进。</p>
<p>整个流程</p>
</div><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined">Colythme</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="http://colythme.github.io/2023/07/14/3D%20Human%20Reconstruction%20Notes/">http://colythme.github.io/2023/07/14/3D%20Human%20Reconstruction%20Notes/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/DigitalHuman/">DigitalHuman</a><a class="post-meta__tags" href="/tags/Avatar/">Avatar</a><a class="post-meta__tags" href="/tags/3D/">3D</a></div><div class="post_share"><div class="social-share" data-image="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2023/07/14/C++%20Basic/"><img class="prev-cover" data-lazy-src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg" onerror="onerror=null;src='/img/404.jpg'"><div class="pagination-info"><div class="label">Previous Post</div><div class="prev_info">C++ Basic</div></div></a></div><div class="next-post pull-right"><a href="/2023/07/14/Social%20Information%20Network%20Analysis%20and%20Engineering/"><img class="next-cover" data-lazy-src="https://i.loli.net/2020/07/13/KhC7N5EDpbt6OM2.jpg" onerror="onerror=null;src='/img/404.jpg'"><div class="pagination-info"><div class="label">Next Post</div><div class="next_info">Social Information Network Analysis and Engineering</div></div></a></div></nav><div class="relatedPosts"><div class="relatedPosts_headline"><i class="fas fa-thumbs-up fa-fw"></i><span> Related Articles</span></div><div class="relatedPosts_list"><div class="relatedPosts_item"><a href="/2023/07/14/基于UE及iClone的数字人驱动/" title="基于UE及iClone的数字人驱动"><img class="relatedPosts_cover" data-lazy-src="https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="far fa-calendar-alt fa-fw"></i> 2023-07-14</div><div class="relatedPosts_title">基于UE及iClone的数字人驱动</div></div></a></div></div></div></article></main><footer id="footer" style="background-image: url(https://i.loli.net/2020/05/01/gkihqEjXxJ5UZ1C.jpg)" data-type="photo"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2023 By Colythme</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><section id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="font_plus" type="button" title="Increase Font Size"><i class="fas fa-plus"></i></button><button id="font_minus" type="button" title="Decrease Font Size"><i class="fas fa-minus"></i></button><button id="translateLink" type="button" title="Switch Between Traditional Chinese And Simplified Chinese">繁</button><button id="darkmode" type="button" title="Switch Between Light And Dark Mode"><i class="fas fa-adjust"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="Setting"><i class="fas fa-cog"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="Back To Top"><i class="fas fa-arrow-up"></i></button></div></section><div class="search-dialog" id="local-search"><div class="search-dialog__title" id="local-search-title">Local search</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="Search for Posts" type="text"/></div></div></div><hr/><div id="local-search-results"><div id="local-hits"></div><div id="local-stats"><div class="local-search-stats__hr" id="hr"><span>Powered by</span> <a target="_blank" rel="noopener" href="https://github.com/wzpan/hexo-generator-search" style="color:#49B1F5;">hexo-generator-search</a></div></div></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div id="search-mask"></div><div><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module" defer></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js" async></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><script src="/js/search/local-search.js"></script><script>var endLoading = function () {
  document.body.style.overflow = 'auto';
  document.getElementById('loading-box').classList.add("loaded")
}
window.addEventListener('load',endLoading)</script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    loader: {
      source: {
        '[tex]/amsCd': '[tex]/amscd'
      }
    },
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, '']
      }
    }
  }
  
  var script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script><script>if (document.getElementsByClassName('mermaid').length) {
  if (window.mermaidJsLoad) mermaid.init()
  else {
    $.getScript('https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js', function () {
      window.mermaidJsLoad = true
      mermaid.initialize({
        theme: 'dark',
      })
      true && mermaid.init()
    })
  }
}</script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><div class="aplayer no-destroy" data-id="2065854146" data-server="netease" data-type="playlist" data-fixed="true" data-mini="true" data-listFolded="false" data-order="random" data-preload="none" data-autoplay="true" muted></div><script src="/js/third-party/activate-power-mode.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = true;
document.body.addEventListener('input', POWERMODE);
</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.css"><script src="https://cdn.jsdelivr.net/npm/aplayer/dist/APlayer.min.js"></script><script src="https://cdn.jsdelivr.net/gh/metowolf/MetingJS@1.2/dist/Meting.min.js"></script><script src="https://cdn.jsdelivr.net/npm/pjax/pjax.min.js"></script><script>var pjax = new Pjax({
  elements: 'a:not([target="_blank"])',
  selectors: [
    'title',
    'meta[name=description]',
    '#config_change',
    '#body-wrap',
    '#rightside-config-hide',
    '#rightside-config-show',
    '.js-pjax'
  ],
  cacheBust: false,
})

document.addEventListener('pjax:complete', function () {
  refreshFn()

  $('script[data-pjax]').each(function () {
    $(this).parent().append($(this).remove())
  })

  GLOBAL_CONFIG.islazyload && lazyLoadInstance.update()

  typeof chatBtnFn === 'function' && chatBtnFn()
  typeof panguInit === 'function' && panguInit()

  if (typeof gtag === 'function') {
    gtag('config', '', {'page_path': window.location.pathname});
  }

  typeof loadMeting === 'function' && document.getElementsByClassName('aplayer').length && loadMeting()

})

document.addEventListener('pjax:send', function () {
  if (window.aplayers) {
    for (let i = 0; i < window.aplayers.length; i++) {
      if (!window.aplayers[i].options.fixed) {
        window.aplayers[i].destroy()
      }
    }
  }

  typeof typed === 'object' && typed.destroy()

  $(window).off('scroll')

  //reset readmode
  $('body').hasClass('read-mode') && $('body').removeClass('read-mode')

  //reset font-size
  $('body').css('font-size') !== originFontSize && $('body').css('font-size', parseFloat(originFontSize))
})</script></div></body></html>